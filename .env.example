# Ollama Configuration
OLLAMA_BASE_URL=http://localhost:11434
EMBED_MODEL=nomic-embed-text
LOCAL_LLM=llama3:8b

# Cloud LLM API Keys (optional - for paid inference)
ANTHROPIC_API_KEY=your_anthropic_api_key_here
GOOGLE_API_KEY=your_google_api_key_here
