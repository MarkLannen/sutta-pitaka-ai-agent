# Ollama Configuration (for local, free inference)
OLLAMA_BASE_URL=http://localhost:11434
EMBED_MODEL=nomic-embed-text

# Cloud LLM API Keys (optional - add any you want to use)
ANTHROPIC_API_KEY=your_anthropic_api_key_here
GOOGLE_API_KEY=your_google_api_key_here
OPENAI_API_KEY=your_openai_api_key_here
